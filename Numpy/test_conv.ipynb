{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先使用 pytorch 中的卷积层进行梯度的验证\n",
    "\n",
    "+ 前向传播:\n",
    "\n",
    "$$\n",
    "oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "$$\n",
    "\n",
    "+ 反向传播:\n",
    "\n",
    "$$\n",
    "kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  3.,  7.,  0.],\n",
      "          [ 0., 15., 36., 44.,  0.],\n",
      "          [ 0., 35., 76., 84.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(300., grad_fn=<SumBackward0>)\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[ 48.,  72.],\n",
      "          [ 72., 108.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "\n",
    "input = Variable(torch.arange(25).view(1, 1, 5, 5).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, padding=3, stride=2, bias=False)\n",
    "net.weight.data.copy_(torch.ones_like(net.weight.data))\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 18.,  26.,  34.,  42.],\n",
      "          [ 82.,  90.,  98., 106.],\n",
      "          [146., 154., 162., 170.],\n",
      "          [210., 218., 226., 234.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(2016., grad_fn=<SumBackward0>)\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[432., 448.],\n",
      "          [560., 576.]]]])\n",
      "tensor([[[[-431., -447.],\n",
      "          [-559., -575.]]]])\n"
     ]
    }
   ],
   "source": [
    "# 当 stride 不是 1 的时候\n",
    "\n",
    "input = Variable(torch.arange(64).view(1, 1, 8, 8).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, bias=False)\n",
    "net.weight.data.copy_(torch.ones_like(net.weight.data))\n",
    "\n",
    "sgd = SGD(net.parameters(), lr=1)\n",
    "\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "\n",
    "sgd.zero_grad()\n",
    "y.backward()\n",
    "sgd.step()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)\n",
    "print(net.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "432 / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现卷积层\n",
    "\n",
    "1. 使用 `np.lib.stride_tricks.as_strided` 与 `tensordot` 函数实现 `im2col` 以及卷积操作.\n",
    "\n",
    "    + 需要考虑到 stride, padding, dilation 等情况\n",
    "    \n",
    "    + 前向传播:\n",
    "\n",
    "    $$\n",
    "    oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "    $$\n",
    "\n",
    "    + 反向传播:\n",
    "\n",
    "    $$\n",
    "    kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[4., 5.],\n",
       "         [6., 7.]]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_padding(input, padding=(0, 0)):\n",
    "    if padding == (0, 0):\n",
    "        return input\n",
    "    B, C, H, W = input.shape\n",
    "    pad = np.zeros((B, C, H + 2 * padding[0], W + 2 * padding[1]))\n",
    "    pad[..., padding[0]:-padding[0], padding[1]:-padding[1]] = input\n",
    "    return pad\n",
    "\n",
    "def make_dilation(input, dilation=(1, 1)):\n",
    "    if dilation == (1, 1):\n",
    "        return input\n",
    "    \n",
    "    B, C, H, W = input.shape\n",
    "    p, q = dilation\n",
    "    oh, ow = p * (H - 1) + 1, q * (W - 1) + 1\n",
    "    pad = np.zeros((B, C, oh, ow))\n",
    "    pad[..., ::p, ::q] = input\n",
    "    return pad\n",
    "\n",
    "def unwrap_padding(input, padding=(0, 0)):\n",
    "    if padding == (0, 0):\n",
    "        return input\n",
    "    p, q = padding\n",
    "    return input[..., p:-p, q:-q]\n",
    "\n",
    "\n",
    "def rotate_kernel(kernel):\n",
    "    return kernel[..., ::-1, ::-1]\n",
    "    \n",
    "a = np.arange(8).reshape(1, 2, 2, 2)\n",
    "a = make_padding(a, (2, 2))\n",
    "unwrap_padding(a, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[0, 2],\n",
       "           [6, 8]]]]]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def im2col(input, ksize, stride=(1, 1), dilation=(1, 1), writeable=False):\n",
    "    \"\"\"\n",
    "    使用 np.lib.stride_tricks.as_strided 实现 im2col, 关键是确认输出结果的 shape 和 strides 两个参数.\n",
    "    input 应该在输入这个函数之前进行 padding\n",
    "    \"\"\"\n",
    "    B, C, H, W = input.shape\n",
    "    kh, kw = ksize\n",
    "    \n",
    "    istrides = input.strides\n",
    "    ostrides = list(istrides + istrides[-2:])\n",
    "    oh = (H - (dilation[0] * (kh - 1) + 1)) / stride[0] + 1\n",
    "    ow = (W - (dilation[1] * (kw - 1) + 1)) / stride[1] + 1\n",
    "    assert int(oh) == oh and int(ow) == ow, 'conv2d not aligned'\n",
    "    oh = int(oh)\n",
    "    ow = int(ow)\n",
    "    ostrides[2] *= stride[0]\n",
    "    ostrides[3] *= stride[1]\n",
    "    ostrides[4] *= dilation[0]\n",
    "    ostrides[5] *= dilation[1]\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(input, shape=(B, C, oh, ow, kh, kw),\n",
    "                                          strides=ostrides,\n",
    "                                          writeable=writeable)\n",
    "\n",
    "a = np.arange(9).reshape(1, 1, 3, 3)\n",
    "im2col(a, (2, 2), stride=(2, 2), dilation=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[[ 8., 12.],\n",
      "         [20., 24.]]]])\n"
     ]
    }
   ],
   "source": [
    "def conv(input, kernel, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    input = make_padding(input, padding)\n",
    "    input = make_dilation(input, dilation)\n",
    "    \n",
    "    x_col = im2col(input, kernel.shape[-2:], stride)\n",
    "    # tensordot 的结果 shape 为 (B, oh, ow, oC)\n",
    "    return np.tensordot(x_col, kernel, axes=[(1, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "\n",
    "## 这个结果和 pytorch 中一样\n",
    "input = np.arange(9).reshape(1, 1, 3, 3)\n",
    "kernel = np.ones((1, 1, 2, 2))\n",
    "out_conv = conv(input, kernel)\n",
    "print(repr(out_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于卷积, 前向传播时有:\n",
    "\n",
    "$$\n",
    "    oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "$$\n",
    "\n",
    "为了实现误差传播, 实现对前一层输入的梯度求解, 上式可以改成:\n",
    "\n",
    "$$\n",
    "h = \\left[(oh - 1)\\times s + 1\\right] + \\left[d\\times (kh - 1)\\right]\n",
    "$$\n",
    "\n",
    "右边第一项表示对 output feature 进行 dilation (设结果为 A) , 但是 dilation 的系数为 `s`, 而第二项表示再对 A 进行 padding, padding 的大小为 `d * (kh - 1)`\n",
    "\n",
    "而对于权重的更新, 上上式可以改为:\n",
    "\n",
    "  $$\n",
    "    kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[6., 6., 6.],\n",
       "          [6., 6., 6.],\n",
       "          [6., 6., 6.]]]]), array([[[[36., 36.],\n",
       "          [36., 36.]]]]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv 的反向传播: 梯度更新和误差传播\n",
    "def conv(input, kernel, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    input = make_padding(input, padding)\n",
    "    input = make_dilation(input, dilation)\n",
    "    \n",
    "    x_col = im2col(input, kernel.shape[-2:], stride)\n",
    "    # tensordot 的结果 shape 为 (B, oh, ow, oC)\n",
    "    return np.tensordot(x_col, kernel, axes=[(1, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def backward_conv(input, kernel, eta, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    h, i = input.shape[-2:]\n",
    "    kernel_grad = np.zeros_like(kernel)\n",
    "    kh, kw = kernel.shape[-2:]\n",
    "    s, _ = stride\n",
    "    p, _ = padding\n",
    "    d, _ = dilation\n",
    "    oh, ow = eta.shape[-2:]\n",
    "    \n",
    "    ieta = eta.copy()\n",
    "    ieta = make_dilation(ieta, stride)\n",
    "    ieta = make_padding(ieta, ((kh - 1), (kw - 1)))\n",
    "    x_col = im2col(ieta, (kh, kw))\n",
    "    input_grad = np.tensordot(x_col, rotate_kernel(kernel), axes=[(1, 4, 5), (1, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "    input_grad = unwrap_padding(input_grad, padding)\n",
    "    \n",
    "    input = make_padding(input, padding)\n",
    "    x_col = im2col(input, (oh, ow), dilation=stride)\n",
    "    kernel_grad = np.tensordot(x_col, eta, axes=[(0, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "    return input_grad, kernel_grad\n",
    "    \n",
    "\n",
    "    \n",
    "input = np.arange(9).reshape(1, 1, 3, 3)\n",
    "kernel = np.arange(4).reshape(*(1, 1, 2, 2))\n",
    "padding = (1, 1)\n",
    "stride = (1, 1)\n",
    "dilation = (1, 1)\n",
    "out_conv = conv(input, kernel, padding=padding, stride=stride, dilation=dilation) \n",
    "print(out_conv.shape)\n",
    "eta = np.ones_like(out_conv)\n",
    "backward_conv(input, kernel, eta, padding=padding, stride=stride, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  3.,  8.,  4.],\n",
      "          [ 9., 19., 25., 10.],\n",
      "          [21., 37., 43., 16.],\n",
      "          [ 6.,  7.,  8.,  0.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(216., grad_fn=<SumBackward0>)\n",
      "tensor([[[[6., 6., 6.],\n",
      "          [6., 6., 6.],\n",
      "          [6., 6., 6.]]]])\n",
      "tensor([[[[36., 36.],\n",
      "          [36., 36.]]]])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.arange(9).view(1, 1, 3, 3).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, padding=1, stride=1, bias=False)\n",
    "shape = net.weight.data.size()\n",
    "net.weight.data.copy_(torch.arange(4).view(shape))\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5. == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试用循环来实现卷积层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]\n",
      "\n",
      "  [[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]\n",
      "\n",
      "  [[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]\n",
      "\n",
      "  [[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]\n",
      "\n",
      "  [[1. 2. 2. 2. 1.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [2. 4. 4. 4. 2.]\n",
      "   [1. 2. 2. 2. 1.]]]]\n",
      "[[[[1488. 1520.]\n",
      "   [1648. 1680.]]]\n",
      "\n",
      "\n",
      " [[[2288. 2320.]\n",
      "   [2448. 2480.]]]\n",
      "\n",
      "\n",
      " [[[3088. 3120.]\n",
      "   [3248. 3280.]]]]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "s = 1\n",
    "d = 1\n",
    "\n",
    "h = w = 5\n",
    "kh = kw = 2\n",
    "N = 2\n",
    "C = 3\n",
    "oC = 1\n",
    "\n",
    "input = np.arange(N * C * h * w, dtype=np.float64).reshape(N, C, h, w)\n",
    "# kernel = np.arange(kh * kw, dtype=np.float64).reshape(1, 1, kh, kw)\n",
    "kernel = np.ones((C, oC, kh, kw))\n",
    "\n",
    "def conv(input, kernel, p, s, d):\n",
    "    input = make_padding(input, (p, p))\n",
    "    \n",
    "    N, C, H, W = input.shape\n",
    "    iC, oC, kh, kw = kernel.shape\n",
    "    assert C == iC, 'channels not aligned'\n",
    "    \n",
    "    oh, ow = (H - (d * (kh - 1) + 1)) // s + 1, (W - (d * (kw - 1) + 1)) // s + 1\n",
    "    out = np.zeros((N, oC, oh, ow))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(oC):\n",
    "            for i in range(oh):\n",
    "                for j in range(ow):\n",
    "                    out[n, c, i, j] = np.sum(input[n, :, i * s : i * s + kh, j * s : j * s + kw] * \\\n",
    "                                             kernel[:, c, ...].squeeze())\n",
    "                    \n",
    "    return out\n",
    "\n",
    "out_conv = conv(input, kernel, p, s, d)\n",
    "\n",
    "def backward_conv(input, kernel, eta, p, s, d):\n",
    "    N, C, h, w = input.shape\n",
    "    iC, oC, kh, kw = kernel.shape\n",
    "    oh, ow = eta.shape[-2:]\n",
    "    input_grad = np.zeros_like(input)\n",
    "    kernel_grad = np.zeros_like(kernel)\n",
    "    \n",
    "    ## 误差更新\n",
    "    ## 由于我是直接对矩阵相乘, 所以这里不需要将 kernel 翻转.\n",
    "    input_grad = make_padding(input_grad, (p, p))\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for i in range(oh):\n",
    "                for j in range(ow):\n",
    "                    input_grad[n, c, i * s : i * s + kh, j * s : j * s + kw] += np.sum(kernel[c, :, ...] * \\\n",
    "                                                                            eta[n, :, i, j], axis=0)\n",
    "    input_grad = unwrap_padding(input_grad, (p, p))\n",
    "\n",
    "    ## 梯度更新\n",
    "    input = make_padding(input, (p, p))\n",
    "    eta = make_dilation(eta, (s, s))\n",
    "    dh, dw = eta.shape[-2:]\n",
    "    for ic in range(iC):\n",
    "        for oc in range(oC):\n",
    "            for i in range(kh):\n",
    "                for j in range(kw):\n",
    "                    kernel_grad[ic, oc, i, j] = np.sum(input[:, ic, i : i + dh, j : j + dw] * eta[:, oc, ...])\n",
    "    return input_grad, kernel_grad\n",
    "\n",
    "\n",
    "eta = np.ones_like(out_conv)\n",
    "input_grad, kernel_grad = backward_conv(input, kernel, eta, p, s, d)\n",
    "print(input_grad)\n",
    "print(kernel_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]],\n",
      "\n",
      "         [[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]],\n",
      "\n",
      "         [[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]],\n",
      "\n",
      "         [[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]],\n",
      "\n",
      "         [[1., 2., 2., 2., 1.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [2., 4., 4., 4., 2.],\n",
      "          [1., 2., 2., 2., 1.]]]])\n",
      "tensor([[[[1488., 1520.],\n",
      "          [1648., 1680.]],\n",
      "\n",
      "         [[2288., 2320.],\n",
      "          [2448., 2480.]],\n",
      "\n",
      "         [[3088., 3120.],\n",
      "          [3248., 3280.]]]])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.arange(N * C * h * w).view(N, C, h, w).float(), requires_grad=True)\n",
    "net = nn.Conv2d(C, oC, (kh, kw), padding=p, stride=s, bias=False)\n",
    "shape = net.weight.data.size()\n",
    "# net.weight.data.copy_(torch.arange(kh * kw).view(shape))\n",
    "net.weight.data.copy_(torch.ones_like(net.weight.data))\n",
    "output = net(input)\n",
    "# print(output)\n",
    "y = output.sum()\n",
    "# print(y)\n",
    "y.backward()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
