{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先使用 pytorch 中的卷积层进行梯度的验证\n",
    "\n",
    "+ 前向传播:\n",
    "\n",
    "$$\n",
    "oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "$$\n",
    "\n",
    "+ 反向传播:\n",
    "\n",
    "$$\n",
    "kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  3.,  7.,  0.],\n",
      "          [ 0., 15., 36., 44.,  0.],\n",
      "          [ 0., 35., 76., 84.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(300., grad_fn=<SumBackward0>)\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[ 48.,  72.],\n",
      "          [ 72., 108.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "\n",
    "input = Variable(torch.arange(25).view(1, 1, 5, 5).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, padding=3, stride=2, bias=False)\n",
    "net.weight.data.copy_(torch.ones_like(net.weight.data))\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 18.,  26.,  34.,  42.],\n",
      "          [ 82.,  90.,  98., 106.],\n",
      "          [146., 154., 162., 170.],\n",
      "          [210., 218., 226., 234.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(2016., grad_fn=<SumBackward0>)\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[432., 448.],\n",
      "          [560., 576.]]]])\n",
      "tensor([[[[-431., -447.],\n",
      "          [-559., -575.]]]])\n"
     ]
    }
   ],
   "source": [
    "# 当 stride 不是 1 的时候\n",
    "\n",
    "input = Variable(torch.arange(64).view(1, 1, 8, 8).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, bias=False)\n",
    "net.weight.data.copy_(torch.ones_like(net.weight.data))\n",
    "\n",
    "sgd = SGD(net.parameters(), lr=1)\n",
    "\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "\n",
    "sgd.zero_grad()\n",
    "y.backward()\n",
    "sgd.step()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)\n",
    "print(net.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "432 / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现卷积层\n",
    "\n",
    "1. 使用 `np.lib.stride_tricks.as_strided` 与 `tensordot` 函数实现 `im2col` 以及卷积操作.\n",
    "\n",
    "    + 需要考虑到 stride, padding, dilation 等情况\n",
    "    \n",
    "    + 前向传播:\n",
    "\n",
    "    $$\n",
    "    oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "    $$\n",
    "\n",
    "    + 反向传播:\n",
    "\n",
    "    $$\n",
    "    kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[4., 5.],\n",
       "         [6., 7.]]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_padding(input, padding=(0, 0)):\n",
    "    if padding == (0, 0):\n",
    "        return input\n",
    "    B, C, H, W = input.shape\n",
    "    pad = np.zeros((B, C, H + 2 * padding[0], W + 2 * padding[1]))\n",
    "    pad[..., padding[0]:-padding[0], padding[1]:-padding[1]] = input\n",
    "    return pad\n",
    "\n",
    "def make_dilation(input, dilation=(1, 1)):\n",
    "    if dilation == (1, 1):\n",
    "        return input\n",
    "    \n",
    "    B, C, H, W = input.shape\n",
    "    p, q = dilation\n",
    "    oh, ow = p * (H - 1) + 1, q * (W - 1) + 1\n",
    "    pad = np.zeros((B, C, oh, ow))\n",
    "    pad[..., ::p, ::q] = input\n",
    "    return pad\n",
    "\n",
    "def unwrap_padding(input, padding=(0, 0)):\n",
    "    if padding == (0, 0):\n",
    "        return input\n",
    "    p, q = padding\n",
    "    return input[..., p:-p, q:-q]\n",
    "\n",
    "\n",
    "def rotate_kernel(kernel):\n",
    "    return kernel[..., ::-1, ::-1]\n",
    "    \n",
    "a = np.arange(8).reshape(1, 2, 2, 2)\n",
    "a = make_padding(a, (2, 2))\n",
    "unwrap_padding(a, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[0, 2],\n",
       "           [6, 8]]]]]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def im2col(input, ksize, stride=(1, 1), dilation=(1, 1), writeable=False):\n",
    "    \"\"\"\n",
    "    使用 np.lib.stride_tricks.as_strided 实现 im2col, 关键是确认输出结果的 shape 和 strides 两个参数.\n",
    "    input 应该在输入这个函数之前进行 padding\n",
    "    \"\"\"\n",
    "    B, C, H, W = input.shape\n",
    "    kh, kw = ksize\n",
    "    \n",
    "    istrides = input.strides\n",
    "    ostrides = list(istrides + istrides[-2:])\n",
    "    oh = (H - (dilation[0] * (kh - 1) + 1)) / stride[0] + 1\n",
    "    ow = (W - (dilation[1] * (kw - 1) + 1)) / stride[1] + 1\n",
    "    assert int(oh) == oh and int(ow) == ow, 'conv2d not aligned'\n",
    "    oh = int(oh)\n",
    "    ow = int(ow)\n",
    "    ostrides[2] *= stride[0]\n",
    "    ostrides[3] *= stride[1]\n",
    "    ostrides[4] *= dilation[0]\n",
    "    ostrides[5] *= dilation[1]\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(input, shape=(B, C, oh, ow, kh, kw),\n",
    "                                          strides=ostrides,\n",
    "                                          writeable=writeable)\n",
    "\n",
    "a = np.arange(9).reshape(1, 1, 3, 3)\n",
    "im2col(a, (2, 2), stride=(2, 2), dilation=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[[ 8., 12.],\n",
      "         [20., 24.]]]])\n"
     ]
    }
   ],
   "source": [
    "def conv(input, kernel, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    input = make_padding(input, padding)\n",
    "    input = make_dilation(input, dilation)\n",
    "    \n",
    "    x_col = im2col(input, kernel.shape[-2:], stride)\n",
    "    # tensordot 的结果 shape 为 (B, oh, ow, oC)\n",
    "    return np.tensordot(x_col, kernel, axes=[(1, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "\n",
    "## 这个结果和 pytorch 中一样\n",
    "input = np.arange(9).reshape(1, 1, 3, 3)\n",
    "kernel = np.ones((1, 1, 2, 2))\n",
    "out_conv = conv(input, kernel)\n",
    "print(repr(out_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于卷积, 前向传播时有:\n",
    "\n",
    "$$\n",
    "    oh = \\frac{h - \\left(d\\times\\left(kh - 1\\right) + 1\\right)}{s} + 1\n",
    "$$\n",
    "\n",
    "为了实现误差传播, 实现对前一层输入的梯度求解, 上式可以改成:\n",
    "\n",
    "$$\n",
    "h = \\left[(oh - 1)\\times s + 1\\right] + \\left[d\\times (kh - 1)\\right]\n",
    "$$\n",
    "\n",
    "右边第一项表示对 output feature 进行 dilation (设结果为 A) , 但是 dilation 的系数为 `s`, 而第二项表示再对 A 进行 padding, padding 的大小为 `d * (kh - 1)`\n",
    "\n",
    "而对于权重的更新, 上上式可以改为:\n",
    "\n",
    "  $$\n",
    "    kh = \\frac{h - \\left(s\\times\\left(oh - 1\\right) + 1\\right)}{d} + 1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "conv2d not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-dbb6cdc72aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mout_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-209-dbb6cdc72aa1>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(input, kernel, padding, stride, dilation)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dilation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# tensordot 的结果 shape 为 (B, oh, ow, oC)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-7355b421cc7b>\u001b[0m in \u001b[0;36mim2col\u001b[0;34m(input, ksize, stride, dilation, writeable)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moh\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conv2d not aligned'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: conv2d not aligned"
     ]
    }
   ],
   "source": [
    "# conv 的反向传播: 梯度更新和误差传播\n",
    "def conv(input, kernel, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    input = make_padding(input, padding)\n",
    "    input = make_dilation(input, dilation)\n",
    "    \n",
    "    x_col = im2col(input, kernel.shape[-2:], stride)\n",
    "    # tensordot 的结果 shape 为 (B, oh, ow, oC)\n",
    "    return np.tensordot(x_col, kernel, axes=[(1, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def backward_conv(input, kernel, eta, padding=(0, 0), stride=(1, 1), dilation=(1, 1)):\n",
    "    h, i = input.shape[-2:]\n",
    "    kernel_grad = np.zeros_like(kernel)\n",
    "    kh, kw = kernel.shape[-2:]\n",
    "    s, _ = stride\n",
    "    p, _ = padding\n",
    "    d, _ = dilation\n",
    "    oh, ow = eta.shape[-2:]\n",
    "    \n",
    "    ieta = eta.copy()\n",
    "    ieta = make_dilation(ieta, stride)\n",
    "    ieta = make_padding(ieta, ((kh - 1), (kw - 1)))\n",
    "    x_col = im2col(ieta, (kh, kw))\n",
    "    input_grad = np.tensordot(x_col, rotate_kernel(kernel), axes=[(1, 4, 5), (1, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "    input_grad = unwrap_padding(input_grad, padding)\n",
    "    \n",
    "    input = make_padding(input, padding)\n",
    "    x_col = im2col(input, (oh, ow), dilation=stride)\n",
    "    kernel_grad = np.tensordot(x_col, eta, axes=[(0, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n",
    "    return input_grad, kernel_grad\n",
    "    \n",
    "\n",
    "    \n",
    "input = np.arange(9).reshape(1, 1, 3, 3)\n",
    "kernel = np.arange(4).reshape(*(1, 1, 2, 2))\n",
    "padding = (1, 1)\n",
    "stride = (2, 2)\n",
    "dilation = (1, 1)\n",
    "out_conv = conv(input, kernel, padding=padding, stride=stride, dilation=dilation) \n",
    "print(out_conv.shape)\n",
    "eta = np.ones_like(out_conv)\n",
    "backward_conv(input, kernel, eta, padding=padding, stride=stride, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[19., 25.],\n",
      "          [37., 43.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "tensor(124., grad_fn=<SumBackward0>)\n",
      "tensor([[[[0., 1., 1.],\n",
      "          [2., 6., 4.],\n",
      "          [2., 5., 3.]]]])\n",
      "tensor([[[[ 8., 12.],\n",
      "          [20., 24.]]]])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.arange(9).view(1, 1, 3, 3).float(), requires_grad=True)\n",
    "net = nn.Conv2d(1, 1, 2, padding=0, stride=1, bias=False)\n",
    "shape = net.weight.data.size()\n",
    "net.weight.data.copy_(torch.arange(4).view(shape))\n",
    "output = net(input)\n",
    "print(output)\n",
    "y = output.sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(input.grad)\n",
    "print(net.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5. == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
